{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from numpy.random import choice\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.graph_embeddings.data_util import MalwareGraph\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = MalwareGraph('../edge_lists/bin_dll_func.csv', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.get_vt_attributes('../VT_Scans.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kas_dict = {}\n",
    "sym_dict = {}\n",
    "for scan, node in zip(mg.responses, mg.scanned_nodes):\n",
    "    if 'scans' in scan:\n",
    "        kaspersky = scan['scans'].get('Kaspersky')\n",
    "        symantec = scan['scans'].get('Symantec')\n",
    "        if kaspersky:\n",
    "            k_result = kaspersky['result']\n",
    "        else:\n",
    "            k_result = None\n",
    "        kas_dict[node] = k_result\n",
    "        if symantec:\n",
    "            s_result = symantec['result']\n",
    "        else:\n",
    "            s_result = None\n",
    "        sym_dict[node] = s_result\n",
    "    else:\n",
    "        kas_dict[node] = None\n",
    "        sym_dict[node] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_labels = []\n",
    "labeled_nodes = []\n",
    "node_comparison = np.zeros((len(kas_dict), len(kas_dict)))\n",
    "for i, node1 in enumerate(kas_dict):\n",
    "    for j, node2 in enumerate(kas_dict):\n",
    "        if kas_dict[node1] and kas_dict[node2]:\n",
    "            node1_label = kas_dict[node1]\n",
    "            node2_label = kas_dict[node2]\n",
    "            \n",
    "            node1_label_split = re.findall(r\"[\\w']+\", node1_label)\n",
    "            node2_label_split = re.findall(r\"[\\w']+\", node2_label)\n",
    "            count = 0\n",
    "            for token in node1_label_split:\n",
    "                if token in node2_label_split:\n",
    "                    count += 1\n",
    "            overlap1 = count / len(node1_label_split)\n",
    "        else:\n",
    "            overlap1 = -1\n",
    "            \n",
    "        if sym_dict[node1] and sym_dict[node2]:\n",
    "            node1_label = sym_dict[node1]\n",
    "            node2_label = sym_dict[node2]\n",
    "            node1_label_split = re.findall(r\"[\\w']+\", node1_label)\n",
    "            node2_label_split = re.findall(r\"[\\w']+\", node2_label)\n",
    "            count = 0\n",
    "            for token in node1_label_split:\n",
    "                if token in node2_label_split:\n",
    "                    count += 1\n",
    "            overlap2 = count / len(node1_label_split)\n",
    "        else:\n",
    "            overlap2 = -1\n",
    "        \n",
    "        \n",
    "        \n",
    "        if overlap1 == -1 and overlap2 == -1:\n",
    "            node_comparison[i,j] = -1\n",
    "        elif overlap1 == -1:\n",
    "#             node_comparison[i,j] = overlap2 / 2\n",
    "            node_comparison[i,j] = overlap2\n",
    "#             node_comparison[i,j] = -1\n",
    "        elif overlap2 == -1:\n",
    "#             node_comparison[i,j] = overlap1 / 2\n",
    "#             node_comparison[i,j] = overlap1\n",
    "            node_comparison[i,j] = -1\n",
    "        else:\n",
    "#             node_comparison[i,j] = (overlap1 + overlap2) / 2\n",
    "#             node_comparison[i,j] = overlap1\n",
    "            node_comparison[i,j] = overlap2\n",
    "            \n",
    "\n",
    "#     if kas_dict[node1] or sym_dict[node1]:\n",
    "    if sym_dict[node1]:\n",
    "        vt_labels.append((kas_dict[node1], sym_dict[node1]))\n",
    "        labeled_nodes.append(node1)\n",
    "\n",
    "node_comparison = pd.DataFrame(node_comparison)\n",
    "cluster_data = node_comparison.where(node_comparison >= 0).dropna(how='all').dropna(axis=1, how='all').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans = KMeans(n_clusters=6)\n",
    "# kmeans.fit(cluster_data)\n",
    "\n",
    "# cluster_labels = kmeans.fit_predict(cluster_data)\n",
    "sc = SpectralClustering(15, \n",
    "#                         affinity='precomputed', \n",
    "                        n_init=100,\n",
    "                        assign_labels='discretize')\n",
    "cluster_labels = sc.fit_predict(cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {}\n",
    "for node, lab in zip(labeled_nodes, cluster_labels):\n",
    "    class_map[node] = lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node, lab in zip(labeled_nodes, cluster_labels):\n",
    "    if lab == 11:\n",
    "        print(lab, sym_dict[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arthrograph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arthrograph.add_nodes_from(mg.files, bipartite=0)\n",
    "arthrograph.add_nodes_from(mg.funcs, bipartite=1)\n",
    "\n",
    "arthrograph.add_edges_from(mg.file_funcs)\n",
    "\n",
    "color_map = []\n",
    "for node in arthrograph:\n",
    "    if node in mg.files:\n",
    "        color_map.append('blue')\n",
    "    elif node in mg.dlls: \n",
    "        color_map.append('green')\n",
    "    else:\n",
    "        color_map.append('purple')\n",
    "\n",
    "nx.draw(arthrograph, node_color = color_map, with_labels = False, node_size=40, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_graph = bipartite.weighted_projected_graph(arthrograph, mg.files, ratio=True)\n",
    "labeled_file_graph = file_graph.subgraph(labeled_nodes)\n",
    "nx.draw(file_graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_file_graph = file_graph.subgraph(labeled_nodes)\n",
    "nx.draw(labeled_file_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSV = cm.get_cmap('hsv', 15)\n",
    "# cs = {0: 'blue', 1: 'red', 2:'green', 3:'yellow', 4:'orange', 5:'purple', 6: 'teal', 7:'', 8, 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(labeled_file_graph)\n",
    "\n",
    "nx.draw(labeled_file_graph, pos, node_color=[HSV(class_map[x]) for x in labeled_file_graph.nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(labeled_file_graph, pos, node_color=[HSV(class_map[x]) for x in labeled_file_graph.nodes])\n",
    "\n",
    "for x in labeled_file_graph.edges(data=True):\n",
    "    ew = x[2].get('weight')*1000\n",
    "    nx.draw_networkx_edges(labeled_file_graph, pos, edgelist=[(x[0], x[1])], width=ew)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = labeled_file_graph.copy()\n",
    "\n",
    "split = 0.1\n",
    "test_nodes = []\n",
    "for i in labeled_file_graph.nodes():\n",
    "    if random.uniform(0, 1) <= split:\n",
    "        test_nodes.append(i)\n",
    "\n",
    "\n",
    "train_graph.remove_nodes_from(test_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(train_graph, node_color=[HSV(class_map[x]) for x in train_graph.nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_graph.nodes()), len(labeled_file_graph.nodes()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deepwalk Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_attr_matrix = nx.attr_matrix(train_graph, normalized=True)[0]\n",
    "nodes_to_ids = {}\n",
    "ids_to_nodes = {}\n",
    "\n",
    "n_id = 0\n",
    "for n in train_graph.nodes():\n",
    "    nodes_to_ids[n] = n_id\n",
    "    ids_to_nodes[n_id] = n\n",
    "    n_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "def choose_neighbor(G_attr_matrix, node):\n",
    "    node_id = nodes_to_ids[node]\n",
    "    norm = G_attr_matrix[node_id][0].tolist()[0]\n",
    "    \n",
    "    if sum(norm) > 0.98:\n",
    "\n",
    "        draw = choice(range(G_attr_matrix.shape[0]), 1, p=norm)[0]\n",
    "    else:\n",
    "        draw = node_id\n",
    "    return ids_to_nodes[draw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks_per_node = 10\n",
    "walk_length = 80\n",
    "walks = []\n",
    "for node in train_graph:\n",
    "    node_walks = 0\n",
    "    while node_walks < walks_per_node:\n",
    "        walk = [node]\n",
    "        cur_node = node\n",
    "        while len(walk) < walk_length:\n",
    "            next_node = choose_neighbor(G_attr_matrix, cur_node)\n",
    "            walk.append(next_node)\n",
    "            cur_node = next_node\n",
    "        walks.append(walk)\n",
    "        node_walks += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(walks, size=16, window=4, min_count=1, sg=1, hs=0, negative=5, ns_exponent=0.75, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kas_dict = {}\n",
    "for scan, node in zip(mg.responses, mg.scanned_nodes):\n",
    "    if 'scans' in scan:\n",
    "        kaspersky = scan['scans'].get('Kaspersky')\n",
    "        if kaspersky:\n",
    "            result = kaspersky['result']\n",
    "        else:\n",
    "            result = None\n",
    "        kas_dict[node] = result\n",
    "    else:\n",
    "        kas_dict[node] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(model.wv.vocab)\n",
    "X = model[vocab]\n",
    "tsne = TSNE(n_components=2, perplexity=10, n_iter=5000)\n",
    "X_transformed = tsne.fit_transform(X)\n",
    "# pca = PCA(n_components=2, svd_solver='full')\n",
    "# X_transformed = pca.fit_transform(X)\n",
    "df = pd.DataFrame(X_transformed, index=vocab, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "classes = []\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    if word in class_map:    \n",
    "        pt_color = HSV(class_map[word])\n",
    "        ax.scatter(pos[0], pos[1], color=pt_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Induction\n",
    "# Hold out some nodes\n",
    "# Train CBOW model - given n neighbors predict location\n",
    "\n",
    "# Generate a number of neighbor samples of that node and feed into model \n",
    "# loss is cosine loss between that and current embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "node_label_counts = Counter([sym_dict[n] for n in labeled_file_graph.nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use neighborhood average for graph induction\n",
    "for node in test_nodes:\n",
    "    av_vector = np.zeros(16)\n",
    "    n_count = 0\n",
    "    t_node_count = 0\n",
    "    neighbor_labels = []\n",
    "    for neighbor in labeled_file_graph.neighbors(node):\n",
    "        try:\n",
    "            av_vector += model.wv[neighbor]\n",
    "            neighbor_labels.append(sym_dict[neighbor])\n",
    "            n_count += 1\n",
    "        except KeyError:\n",
    "            t_node_count += 1\n",
    "    av_vector /= n_count\n",
    "    sims = model.wv.similar_by_vector(av_vector, topn=5)\n",
    "    \n",
    "    print(sym_dict[node], [sym_dict[n[0]] for n in sims])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neighbors = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for node in train_graph.nodes:\n",
    "    neighbors = list(train_graph.neighbors(node))\n",
    "    if len(neighbors) > 0:\n",
    "        for i in range(400):\n",
    "            neighbor_samples = []\n",
    "            while len(neighbor_samples) < num_neighbors:\n",
    "                neighbor_sample = choice(neighbors, 1)\n",
    "                try:\n",
    "                    neighbor_samples.append(model.wv[neighbor_sample]) \n",
    "                except KeyError:\n",
    "                    continue\n",
    "            X_train.append(neighbor_samples)\n",
    "            y_train.append(nodes_to_ids[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "final_test_nodes = []\n",
    "for node in test_nodes:\n",
    "    neighbors = list(labeled_file_graph.neighbors(node))\n",
    "    if len(neighbors) > 0:\n",
    "        neighbor_samples = []\n",
    "        while len(neighbor_samples) < num_neighbors:\n",
    "            neighbor_sample = choice(neighbors, 1)\n",
    "            try:\n",
    "                neighbor_samples.append(model.wv[neighbor_sample]) \n",
    "            except KeyError:\n",
    "                continue\n",
    "        X_test.append(neighbor_samples)\n",
    "        final_test_nodes.append(node)\n",
    "X_test = np.squeeze(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.squeeze(np.array(X_train))\n",
    "y_train = np.array(y_train)\n",
    "s = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(s)\n",
    "X_train=X_train[s,:]\n",
    "\n",
    "n_values = np.max(y_train) + 1\n",
    "y_train_oh = np.eye(n_values)[y_train]\n",
    "y_train_oh = y_train_oh[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = Sequential()\n",
    "cbow_model.add(Dense(16, activation='relu', input_shape=(num_neighbors, 16,)))\n",
    "cbow_model.add(Dense(num_neighbors/4, activation='relu'))\n",
    "cbow_model.add(Flatten(name='embedding'))\n",
    "cbow_model.add(Dense(len(train_graph.nodes), activation='softmax'))\n",
    "\n",
    "cbow_model.summary()\n",
    "\n",
    "cbow_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = cbow_model.fit(X_train, y_train_oh,\n",
    "                    batch_size=512,\n",
    "                    epochs=30,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "intermediate_layer_model = Model(inputs=cbow_model.input,\n",
    "                                 outputs=cbow_model.get_layer('embedding').output)\n",
    "intermediate_output = intermediate_layer_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for vec, node in zip(intermediate_output, final_test_nodes):\n",
    "    sims = model.wv.similar_by_vector(vec, topn=5)\n",
    "    for n in sims:\n",
    "        if class_map[node] == class_map[n[0]]:\n",
    "            correct += 1\n",
    "\n",
    "print(correct/(5*len(final_test_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
